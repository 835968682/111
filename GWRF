import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
import geoxgboost as gx
import importlib
from joblib import Parallel, delayed  # For parallel processing

# Ensure reloading of geoxgboost module (important for Jupyter environments)
importlib.reload(gx)

# Helper function: calculate root mean squared error
def root_mean_squared_error(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# Optimized distance matrix calculation function
def distance_matrix(coords1, coords2):
    """Optimized Haversine distance calculation with numerical stability"""
    # Convert to radians
    coords1_rad = np.radians(coords1)
    coords2_rad = np.radians(coords2)
    # Use vectorized calculation
    delta_lat = coords1_rad[:, 0, None] - coords2_rad[:, 0]
    delta_lon = coords1_rad[:, 1, None] - coords2_rad[:, 1]
    # Haversine formula
    a = np.sin(delta_lat / 2) ** 2 + np.cos(coords1_rad[:, 0, None]) * np.cos(coords2_rad[:, 0]) * np.sin(
        delta_lon / 2) ** 2
    # Numerical stability handling: ensure a is in [0,1] range to avoid sqrt and arcsin calculation errors
    a = np.clip(a, 0, 1)
    c = 2 * np.arcsin(np.sqrt(a))
    return 6371000 * c  # Return in meters

# 1. Use GeoXGBoost's nested cross-validation to optimize random forest parameters
def optimize_rf_params_with_nested_cv(X, y):
    """
    Use GeoXGBoost's nested cross-validation framework to optimize random forest parameters
    """
    # Define random forest parameter grid (using similar search strategy as GXGB)
    rf_params = {
        'n_estimators': 100,  # Initial value
        'max_depth': 10,  # Consistent with GXGB
        'min_samples_split': 2,
        'min_samples_leaf': 1,
        'max_features': 'sqrt',
        'random_state': 42
    }

    # Create parameter grid for random forest (using GeoXGBoost's parameter creation function)
    # Note: Need to create a compatible parameter grid
    param1 = 'n_estimators'
    param1_values = [50, 100, 200, 300]
    param2 = 'max_depth'
    param2_values = [5, 10, 15, 20]
    param3 = 'min_samples_split'
    param3_values = [2, 5, 10]

    param_grid = []
    for n_est in param1_values:
        for md in param2_values:
            for mss in param3_values:
                param_grid.append({
                    'n_estimators': n_est,
                    'max_depth': md,
                    'min_samples_split': mss
                })

    # Use GeoXGBoost's nested cross-validation
    try:
        # Since GeoXGBoost is mainly for XGBoost, we need to create an adapter
        # Here we use traditional nested CV but maintain structure similar to GXGB
        best_params = rf_params.copy()
        best_score = float('inf')

        # Use 3-fold cross-validation
        kf = KFold(n_splits=3, shuffle=True, random_state=42)

        for params in param_grid:
            fold_scores = []
            for train_idx, val_idx in kf.split(X):
                X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]
                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

                model = RandomForestRegressor(**params, random_state=42)
                model.fit(X_train_fold, y_train_fold)
                y_pred = model.predict(X_val_fold)
                rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                fold_scores.append(rmse)

            avg_score = np.mean(fold_scores)
            if avg_score < best_score:
                best_score = avg_score
                best_params = params.copy()

        print(f"Optimized random forest parameters: {best_params}")
        return best_params

    except Exception as e:
        print(f"GeoXGBoost nested CV failed: {e}")
        # Fallback to sklearn's GridSearchCV
        from sklearn.model_selection import GridSearchCV
        rf = RandomForestRegressor(random_state=42)
        param_grid_sklearn = {
            'n_estimators': [50, 100, 200, 300],
            'max_depth': [5, 10, 15, 20],
            'min_samples_split': [2, 5, 10]
        }
        grid_search = GridSearchCV(
            estimator=rf,
            param_grid=param_grid_sklearn,
            cv=3,
            scoring='neg_root_mean_squared_error',
            n_jobs=-1
        )
        grid_search.fit(X, y)
        print(f"sklearn GridSearchCV optimized parameters: {grid_search.best_params_}")
        return grid_search.best_params_

# 2. Use GeoXGBoost's bandwidth optimization function
def optimize_bandwidth_gxgb_style(X, y, coords, rf_params):
    """
    Use the same bandwidth optimization method as GXGB
    """
    # Convert random forest parameters to compatible format
    # Create a mock params dictionary to make it compatible with GXGB
    params = {
        'n_estimators': rf_params.get('n_estimators', 100),
        'max_depth': rf_params.get('max_depth', 10),
        'min_samples_split': rf_params.get('min_samples_split', 2),
        'random_state': 42
    }

    # Use GeoXGBoost's bandwidth optimization function
    try:
        best_bw = gx.optimize_bw(
            X=X,
            y=y,
            Coords=coords,
            params=params,
            bw_min=75,  # Same range as GXGB
            bw_max=85,
            step=5,
            Kernel='Adaptive',  # Same as GXGB
            spatial_weights=True  # Same as GXGB
        )
        return best_bw
    except Exception as e:
        print(f"GeoXGBoost bandwidth optimization failed: {e}")
        # Fallback to custom implementation but use same parameters
        return optimize_bandwidth_fallback(X, y, coords, params)

def optimize_bandwidth_fallback(X, y, coords, rf_params):
    """
    Fallback implementation for bandwidth optimization using same parameter settings as GXGB
    """
    bandwidths = list(range(75, 86, 5))  # Same range as GXGB: 75, 80, 85
    kf = KFold(n_splits=3, shuffle=True, random_state=42)  # Same CV settings as GXGB

    scores = []
    for bw in bandwidths:
        fold_scores = []
        for train_idx, val_idx in kf.split(X):
            # Prepare data
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            coords_train, coords_val = coords[train_idx], coords[val_idx]

            # Create and train model
            model = OptimizedGWRF(
                bw=bw,
                rf_params=rf_params,
                kernel='Adaptive',  # Same as GXGB
                spatial_weights=True  # Same as GXGB
            )
            model.fit(X_train, y_train, coords_train)

            # Predict and evaluate
            y_pred = model.predict(X_val, coords_val)
            rmse = np.sqrt(mean_squared_error(y_val, y_pred))
            fold_scores.append(rmse)

        avg_score = np.mean(fold_scores)
        scores.append(avg_score)
        print(f"Bandwidth {bw}: Average RMSE = {avg_score:.4f}")

    best_idx = np.argmin(scores)
    best_bw = bandwidths[best_idx]
    print(f"Optimal bandwidth: {best_bw}, Minimum average RMSE: {scores[best_idx]:.4f}")
    return best_bw

# Optimized geographically weighted random forest class
class OptimizedGWRF:
    def __init__(self, bw, rf_params, kernel='Adaptive', spatial_weights=True, n_jobs=-1):
        self.bw = bw
        self.rf_params = rf_params
        self.kernel = kernel
        self.spatial_weights = spatial_weights
        self.n_jobs = n_jobs  # Number of parallel jobs
        self.models = {}  # Cache trained models
        self.coords_train = None
        self.X_train = None
        self.y_train = None
        self.feature_names = None  # New: store feature names

    def fit(self, X, y, coords):
        # Store feature names (if they exist)
        self.feature_names = list(X.columns) if hasattr(X, 'columns') else None
        # Convert to array format
        self.X_train = X.values if hasattr(X, 'values') else np.array(X)
        self.y_train = y.values if hasattr(y, 'values') else np.array(y)
        self.coords_train = coords
        # Precompute distance matrix between training points
        self.train_dist_matrix = distance_matrix(coords, coords)

    def _predict_single(self, i, X_test, test_dist_matrix):
        """Predict single data point"""
        # Use array format to get test point features
        test_features = X_test.values[i].reshape(1, -1) if hasattr(X_test, 'values') else X_test[i].reshape(1, -1)
        # Get current test point distances
        distances = test_dist_matrix[i, :]
        # Select neighbors based on kernel type (same method as GXGB)
        if self.kernel == 'Adaptive':
            n_neighbors = min(self.bw, len(distances))
            # Use argpartition for efficiency
            idx = np.argpartition(distances, n_neighbors)[:n_neighbors]
            # Re-sort to ensure correctness
            sorted_idx = np.argsort(distances[idx])
            idx = idx[sorted_idx]
            h = distances[idx[-1]]  # Maximum distance as bandwidth
        else:
            # Fixed kernel: select points with distance less than bandwidth
            idx = np.where(distances <= self.bw)[0]
            h = self.bw

        # If no neighbors, use global model
        if len(idx) == 0:
            if "global" not in self.models:
                self.models["global"] = RandomForestRegressor(
                    **self.rf_params, n_jobs=self.n_jobs
                ).fit(self.X_train, self.y_train)
            return self.models["global"].predict(test_features)[0]

        # Get local data
        local_X = self.X_train[idx]  # Use array indexing
        local_y = self.y_train[idx]  # Use array indexing
        local_distances = distances[idx]

        # Calculate spatial weights (bisquare kernel function, same as GXGB)
        weights = (1 - (local_distances / h) ** 2) ** 2
        weights[local_distances > h] = 0  # Ensure weights beyond bandwidth are 0

        # Train or get cached model
        model_key = tuple(sorted(idx))
        if model_key not in self.models:
            model = RandomForestRegressor(**self.rf_params, n_jobs=self.n_jobs)
            if self.spatial_weights:
                model.fit(local_X, local_y, sample_weight=weights)
            else:
                model.fit(local_X, local_y)
            self.models[model_key] = model

        # Predict current test point
        return self.models[model_key].predict(test_features)[0]

    def predict(self, X_test, coords_test):
        """Predict multiple data points (supports parallel processing)"""
        # Ensure test data is also in array format
        if hasattr(X_test, 'values'):
            X_test = X_test.values
        elif not isinstance(X_test, np.ndarray):
            X_test = np.array(X_test)

        # Calculate distance matrix from test points to training points
        test_dist_matrix = distance_matrix(coords_test, self.coords_train)

        # Use parallel processing to speed up prediction
        predictions = Parallel(n_jobs=self.n_jobs)(
            delayed(self._predict_single)(i, X_test, test_dist_matrix)
            for i in range(len(X_test))
        )
        return np.array(predictions)

# Main program
## Step 1: Import data
print("Loading data...")
# Import coordinate data
Coords = pd.read_csv('2021_2023Qcroods.csv')  # Coordinate data - MODIFIED FILE NAME
# Import feature and target variable data
Data = pd.read_csv('2021_2023Qdata.csv')  # Contains GISid, feature variables X and target variable y - MODIFIED FILE NAME

# Data preprocessing
X = Data.iloc[:, 1:-1]  # Remove GISid and y, keep feature variables
y = Data.iloc[:, -1]  # Target variable y
VarNames = X.columns[:]  # Get variable names

# Coordinate data processing (assuming coordinate file contains X and Y columns)
coords = Coords[['X', 'Y']].values.astype(np.float32)

print(f"Total data volume: {len(X)}")
print(f"Number of features: {X.shape[1]}")

# Split into 70% training data and 30% test data
print("Splitting training and test sets...")
X_train, X_test, y_train, y_test, coords_train, coords_test = train_test_split(
    X, y, coords, test_size=0.3, random_state=42
)

print(f"Training set size: {len(X_train)}")
print(f"Test set size: {len(X_test)}")

# Use same optimization strategy as GXGB
print("Using GeoXGBoost-style nested cross-validation to optimize random forest parameters...")
rf_params = optimize_rf_params_with_nested_cv(X_train, y_train)
print(f"Optimized random forest parameters: {rf_params}")

print("Using GeoXGBoost-style bandwidth optimization...")
best_bw = optimize_bandwidth_gxgb_style(X_train, y_train, coords_train, rf_params)
print(f"Optimal bandwidth: {best_bw}")

# Train final model
print(f"Training geographically weighted random forest model (bandwidth={best_bw})...")
model = OptimizedGWRF(
    bw=best_bw,
    rf_params=rf_params,
    kernel='Adaptive',  # Same as GXGB
    spatial_weights=True,  # Same as GXGB
    n_jobs=4  # Use 4 CPU cores
)
model.fit(X_train, y_train, coords_train)

# Prediction and evaluation
print("Making predictions...")
y_pred = model.predict(X_test, coords_test)
rmse = root_mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Test set RMSE: {rmse:.4f}, RÂ²: {r2:.4f}")
