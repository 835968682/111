import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
import random
from scipy import stats
import geopandas as gpd
from matplotlib.font_manager import FontProperties


# 设置随机种子以确保结果可复现
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


set_seed(42)


# 定义地理空间感知的深度学习模型
class GeoAwareNN(nn.Module):
    def __init__(self, input_dim, spatial_dim, hidden_dim=128, num_layers=3, dropout=0.2):
        super(GeoAwareNN, self).__init__()

        # 特征处理网络
        feature_layers = []
        feature_layers.append(nn.Linear(input_dim, hidden_dim))
        feature_layers.append(nn.ReLU())
        feature_layers.append(nn.Dropout(dropout))

        for _ in range(num_layers - 1):
            feature_layers.append(nn.Linear(hidden_dim, hidden_dim))
            feature_layers.append(nn.ReLU())
            feature_layers.append(nn.Dropout(dropout))

        self.feature_net = nn.Sequential(*feature_layers)

        # 空间处理网络
        spatial_layers = []
        spatial_layers.append(nn.Linear(spatial_dim, hidden_dim))
        spatial_layers.append(nn.ReLU())
        spatial_layers.append(nn.Dropout(dropout))

        for _ in range(num_layers - 1):
            spatial_layers.append(nn.Linear(hidden_dim, hidden_dim))
            spatial_layers.append(nn.ReLU())
            spatial_layers.append(nn.Dropout(dropout))

        self.spatial_net = nn.Sequential(*spatial_layers)

        # 合并网络
        merge_layers = []
        merge_layers.append(nn.Linear(hidden_dim * 2, hidden_dim))
        merge_layers.append(nn.ReLU())
        merge_layers.append(nn.Dropout(dropout))

        for _ in range(num_layers - 1):
            merge_layers.append(nn.Linear(hidden_dim, hidden_dim))
            merge_layers.append(nn.ReLU())
            merge_layers.append(nn.Dropout(dropout))

        merge_layers.append(nn.Linear(hidden_dim, 1))

        self.merge_net = nn.Sequential(*merge_layers)

        # 初始化权重
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, x, coords):
        x_features = self.feature_net(x)
        x_spatial = self.spatial_net(coords)
        x_combined = torch.cat([x_features, x_spatial], dim=1)
        output = self.merge_net(x_combined)
        return output


# 自定义数据集类
class GeoDataset(Dataset):
    def __init__(self, features, coords, targets):
        self.features = torch.FloatTensor(features)
        self.coords = torch.FloatTensor(coords)
        self.targets = torch.FloatTensor(targets)

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        return {
            'features': self.features[idx],
            'coords': self.coords[idx],
            'targets': self.targets[idx]
        }


# 训练函数
def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=100, patience=20):
    best_val_loss = float('inf')
    best_model = None
    patience_counter = 0

    for epoch in range(epochs):
        # 训练阶段
        model.train()
        train_loss = 0.0

        for batch in train_loader:
            features = batch['features'].to(device)
            coords = batch['coords'].to(device)
            targets = batch['targets'].to(device)

            outputs = model(features, coords)
            loss = criterion(outputs.squeeze(), targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        train_loss /= len(train_loader)

        # 验证阶段
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                features = batch['features'].to(device)
                coords = batch['coords'].to(device)
                targets = batch['targets'].to(device)

                outputs = model(features, coords)
                loss = criterion(outputs.squeeze(), targets)
                val_loss += loss.item()

        val_loss /= len(val_loader)

        # 早停机制
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping after {epoch + 1} epochs")
                break

        if (epoch + 1) % 20 == 0:
            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')

    # 加载最佳模型
    model.load_state_dict(best_model)
    return model


# 预测函数
def predict_model(model, data_loader, device):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch in data_loader:
            features = batch['features'].to(device)
            coords = batch['coords'].to(device)
            targets = batch['targets'].to(device)

            outputs = model(features, coords)
            predictions.extend(outputs.cpu().numpy().flatten())
            actuals.extend(targets.cpu().numpy())

    return np.array(predictions), np.array(actuals)


def main():
    # 1. 加载数据（按照你的格式）
    print("加载数据...")
    # 导入坐标数据
    Coords = pd.read_csv('2017_2020Qcroods.csv')  # 坐标数据
    # 导入特征和目标变量数据
    Data = pd.read_csv('2017_2020Qdata.csv')  # 包含GISid, 特征变量X和目标变量y

    # 数据预处理
    X = Data.iloc[:, 1:-1]  # 移除GISid和y，保留特征变量
    y = Data.iloc[:, -1]  # 目标变量y
    VarNames = X.columns[:]  # 获取变量名称

    # 坐标数据处理（假设坐标文件包含latitude和longitude列）
    coords = Coords[['X', 'Y']].values.astype(np.float32)

    print(f"数据总量: {len(X)}")
    print(f"特征数量: {X.shape[1]}")
    print(f"特征列名: {list(VarNames)}")

    # 按70%训练数据和30%测试数据进行划分
    print("划分训练集和测试集...")
    X_train, X_test, y_train, y_test, coords_train, coords_test = train_test_split(
        X, y, coords, test_size=0.3, random_state=42
    )

    print(f"训练集大小: {len(X_train)}")
    print(f"测试集大小: {len(X_test)}")

    # 数据探索
    print(f"\n数据探索:")
    print(f"Y变量统计: 均值={y.mean():.4f}, 标准差={y.std():.4f}, 最小值={y.min():.4f}, 最大值={y.max():.4f}")

    # 检查特征相关性
    corr_matrix = np.corrcoef(X.values.T, y.values)
    corr_with_y = corr_matrix[:-1, -1]
    print(f"\n特征与Y的相关性:")
    for i, col in enumerate(VarNames):
        print(f"{col}: {corr_with_y[i]:.4f}")

    # 2. 特征标准化
    feature_scaler = StandardScaler()
    X_train_scaled = feature_scaler.fit_transform(X_train.values)
    X_test_scaled = feature_scaler.transform(X_test.values)

    coord_scaler = StandardScaler()
    coords_train_scaled = coord_scaler.fit_transform(coords_train)
    coords_test_scaled = coord_scaler.transform(coords_test)

    target_scaler = StandardScaler()
    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()
    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()

    # 3. 创建数据集和数据加载器
    train_dataset = GeoDataset(X_train_scaled, coords_train_scaled, y_train_scaled)
    test_dataset = GeoDataset(X_test_scaled, coords_test_scaled, y_test_scaled)

    batch_size = 32
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # 4. 设置设备
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"使用设备: {device}")

    # 5. 初始化模型
    model = GeoAwareNN(
        input_dim=X_train.shape[1],
        spatial_dim=coords_train.shape[1],
        hidden_dim=128,
        num_layers=3,
        dropout=0.1
    ).to(device)

    # 6. 定义损失函数和优化器
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

    # 7. 训练模型
    print("\n开始训练地理神经网络模型...")
    model = train_model(
        model, train_loader, test_loader, criterion, optimizer, device, epochs=500, patience=50
    )

    # 8. 评估模型
    print("\n评估模型...")
    train_predictions_scaled, train_actuals_scaled = predict_model(model, train_loader, device)
    test_predictions_scaled, test_actuals_scaled = predict_model(model, test_loader, device)

    # 反标准化预测值和实际值
    train_predictions = target_scaler.inverse_transform(train_predictions_scaled.reshape(-1, 1)).flatten()
    train_actuals = target_scaler.inverse_transform(train_actuals_scaled.reshape(-1, 1)).flatten()
    test_predictions = target_scaler.inverse_transform(test_predictions_scaled.reshape(-1, 1)).flatten()
    test_actuals = target_scaler.inverse_transform(test_actuals_scaled.reshape(-1, 1)).flatten()

    # 计算评估指标
    train_mse = mean_squared_error(train_actuals, train_predictions)
    train_rmse = np.sqrt(train_mse)
    train_r2 = r2_score(train_actuals, train_predictions)

    test_mse = mean_squared_error(test_actuals, test_predictions)
    test_rmse = np.sqrt(test_mse)
    test_r2 = r2_score(test_actuals, test_predictions)

    train_mae = mean_absolute_error(train_actuals, train_predictions)
    test_mae = mean_absolute_error(test_actuals, test_predictions)

    print(f"训练集 - MSE: {train_mse:.4f}, RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}, MAE: {train_mae:.4f}")
    print(f"测试集 - MSE: {test_mse:.4f}, RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}, MAE: {test_mae:.4f}")
